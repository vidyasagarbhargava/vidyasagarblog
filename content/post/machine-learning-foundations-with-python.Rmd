---
title: 'Machine Learning foundations with python '
author: Vidyasagar Bhargava
date: '2019-12-25'
slug: machine-learning-foundations-with-python
categories:
  - Tutorials
tags:
  - machine learning
  - python
keywords:
  - tech
---

There are three distinct types of problems in the field machine learning.  

1. **Unsupervised Learning** :- So in this type we only have inputs and we try to model the distribution in order to better understand the underlying structure.  
For example :-
  * We have census data and try to split segment people into different unknown categories.  
 
2. **Supervised Learning** :- Here we create a model that can predict an output from an input.
Output often known as labels and input as features can take different forms.  
For example :- 
  * We take as input different features about a house such as location, number of rooms etc.  and try to predict the price.  
  * Taking in an image as input and outputting the probability that there is a car in the image
  * Taking in a sequence of words and outputting a probability distribution over the next word

3. **Reinforcement Learning**:- We have an agent in enviornment which has to learn what actions to take to maximize the reward.  
For example :- 
  * We try to get an algorithm to learn  how to win at tic-tac-toe autonomously.
 
 
 
# Data
The first thing in machine learning problem is the data. We don't any data right now. So we will create our own data.For that we create a
function that generates some artificial data. The function should return any noisy linear data of size m.(which is a parameter of the function.)  

> Linear functions are good simple function that we can test our learning algorithm however data collected in real world is often has much more complex correlations.


```{python}
import numpy as np
import matplotlib.pyplot as plt

def sample_linear_data(m=20): 
    ground_truth_w = 2.3 # slope
    ground_truth_b = -8 #intercept
    X = np.random.randn(m)*2
    Y = ground_truth_w*X + ground_truth_b + 0.2*np.random.randn(m)
    return X, Y #returns X (the input) and Y (labels)

def plot_data(X, Y):
    plt.figure()
    plt.scatter(X, Y, c='r')
    plt.xlabel('X')
    plt.ylabel('Y')
    plt.show()
    
m = 10
X, Y = sample_linear_data(m)
print('X:',X, '\n')
print('Y:',Y, '\n')
plot_data(X, Y)
```
 
 
 So now we have created our artificial data. Now lets create our own model and use it to make prediction on our data. We will be using a simple linear model which has a single weight and bias. 
 
 
```{python}
class LinearHypothesis:
    def __init__(self): #initalize parameters 
        self.w = np.random.randn() #weight
        self.b = np.random.randn() #bias
    def __call__(self, X): #how do we calculate output from an input in our model?
        ypred = self.w*X + self.b
        return ypred
    def update_params(self, new_w, new_b):
        self.w = new_w
        self.b = new_b

H = LinearHypothesis()
y_hat = H(X)
print('Input:',X, '\n')
print('W:', H.w, 'B:', H.b, '\n')
print('Prediction:', y_hat, '\n')
```
 
**Lets visualise our hypothesis vs the labels** 

```{python}
def plot_h_vs_y(X, y_hat, Y):
    plt.figure()
    plt.scatter(X, Y, c='r', label='Label')
    plt.scatter(X, y_hat, c='b', label='Hypothesis', marker='x')
    plt.legend()
    plt.xlabel('X')
    plt.ylabel('Y')
    plt.show()
    
plot_h_vs_y(X, y_hat, Y)
```
 
 
 **How good our model is ?** 
 
 So lets calculate the cost. In this case we will use mean squared error.

  
```{python}
#loss function 
def L(y_hat, labels):
    cost = np.sum(np.square(y_hat-labels))/(m)
    return cost
cost = L(y_hat, Y)
print(cost)

```
 
 
**How to find the right weight values for our model?**  
There multiple technique like random search, grid search, gradient descent etc.  


**Random Search** 

```{python}
def random_search(n_samples):
    best_weights = None
    best_bias = None
    lowest_cost=100000 #initialize it very high
    for i in range(n_samples):
        H.update_params(np.random.randn(), np.random.randn())
        y_hat = H(X)
        cost = L(y_hat, Y)
        if cost<lowest_cost:
            lowest_cost=cost
            best_weights = H.w
            best_bias = H.b
    print('Lowest cost of', lowest_cost, 'achieved with weight of', best_weights, 'and bias of', best_bias)
    return lowest_cost, best_weights
    
lowest_cost, best_weights = random_search(1000)
```

```{python}

plot_h_vs_y(X, H(X), Y)
```
 
 
 **Grid Search**
```{python}
from itertools import permutations
def generate_grid_search_values(n_params, n_samples=100, minval=-2.5, maxval=2.5):
    n_samples_per_param = np.power(n_samples, 1/n_params).round()
    param_values = np.linspace(-2.5, 2.5, n_samples_per_param)
    grid_samples = permutations(param_values, n_params)
    return grid_samples

def grid_search(grid_search_values):
    best_weights = None
    best_bias = None
    lowest_cost=100000 #initialize it very high
    for search_val in grid_search_values:
        H.update_params(search_val[0], search_val[1])
        y_hat = H(X)
        cost = L(y_hat, Y)
        if cost<lowest_cost:
            lowest_cost=cost
            best_weights = H.w
            best_bias = H.b
    print('Lowest cost of', lowest_cost, 'achieved with weight of', best_weights, 'and bias of', best_bias)
    return lowest_cost, best_weights
```
 
 
```{python}
grid_search_values = list(generate_grid_search_values(2, n_samples=1000))
lowest_cost, best_weights = grid_search(grid_search_values)
plot_h_vs_y(X, H(X), Y)
```
 
**Gradient Descent**  
Gradient descent is another optimization algorithm that we could use. We can use gradient descent when our model is a differentiable function. Linear functions are pretty simple to differentiate hence why we can use it here.  

The algorithm starts by randomly initializing our parameters. We then calculate the cost of those parameters and the derivative of our cost w.r.t each parameter. This tells us the direction of steepest ascent. We update each parameter value by taking a step in the opposite direction, proportional to the learning rate.  
 
```{python}
class LinearHypothesis:
    def __init__(self): 
        self.w = np.random.randn() #weight
        self.b = np.random.randn() #bias
    def __call__(self, X): #how do we calculate output from an input in our model?
        y_hat = self.w*X + self.b
        return y_hat
    def update_params(self, new_w, new_b):
        self.w = new_w
        self.b = new_b
    def calc_deriv(self, X, y_hat, labels):
        diffs = y_hat-labels
        dLdw = 2*np.array(np.sum(diffs*X)/m)
        dLdb = 2*np.array(np.sum(diffs)/m)
        return dLdw, dLdb
```
 
```{python}
H = LinearHypothesis()
y_hat = H(X)
dLdw, dLdb = H.calc_deriv(X, y_hat, Y)
print(dLdw, dLdb)
```
 
 
Now that we can complete the derivatives, complete the train function below to iteratively improve our parameter estimes to minimize the cost
 
```{python}
num_epochs = 200
learning_rate = 0.1
H = LinearHypothesis()
```
 
 
```{python}
def train(num_epochs, X, Y, H, L, plot_cost_curve=False):
    all_costs = []
    for e in range(num_epochs):
        y_hat = H(X)
        cost = L(y_hat, Y)
        dLdw, dLdb = H.calc_deriv(X, y_hat, Y)
        new_w = H.w - learning_rate*dLdw
        new_b = H.b - learning_rate*dLdb
        H.update_params(new_w, new_b)
        all_costs.append(cost)
    if plot_cost_curve:
        plt.figure()
        plt.ylabel('Cost')
        plt.xlabel('Epoch')
        plt.plot(all_costs)
    print('Final cost:', cost)
    print('Weight values:', H.w)
    print('Bias values:', H.b)
    #return cost, H.w
```
 
```{python}
train(num_epochs, X, Y, H, L, plot_cost_curve=True)
```
 
```{python}
plot_h_vs_y(X, H(X), Y)

```
 
 