---
title: 'Machine Learning foundations with python '
author: Vidyasagar Bhargava
date: '2019-12-25'
slug: machine-learning-foundations-with-python
categories:
  - Tutorials
tags:
  - machine learning
  - python
keywords:
  - tech
---



<p>There are three distinct types of problems in the field machine learning.</p>
<ol style="list-style-type: decimal">
<li><strong>Unsupervised Learning</strong> :- So in this type we only have inputs and we try to model the distribution in order to better understand the underlying structure.<br />
For example :-</li>
</ol>
<ul>
<li>We have census data and try to split segment people into different unknown categories.</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li><strong>Supervised Learning</strong> :- Here we create a model that can predict an output from an input.
Output often known as labels and input as features can take different forms.<br />
For example :-</li>
</ol>
<ul>
<li>We take as input different features about a house such as location, number of rooms etc. and try to predict the price.<br />
</li>
<li>Taking in an image as input and outputting the probability that there is a car in the image</li>
<li>Taking in a sequence of words and outputting a probability distribution over the next word</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li><strong>Reinforcement Learning</strong>:- We have an agent in enviornment which has to learn what actions to take to maximize the reward.<br />
For example :-</li>
</ol>
<ul>
<li>We try to get an algorithm to learn how to win at tic-tac-toe autonomously.</li>
</ul>
<div id="data" class="section level1">
<h1>Data</h1>
<p>The first thing in machine learning problem is the data. We donâ€™t any data right now. So we will create our own data.For that we create a
function that generates some artificial data. The function should return any noisy linear data of size m.(which is a parameter of the function.)</p>
<blockquote>
<p>Linear functions are good simple function that we can test our learning algorithm however data collected in real world is often has much more complex correlations.</p>
</blockquote>
<pre class="python"><code>import numpy as np
import matplotlib.pyplot as plt

def sample_linear_data(m=20): 
    ground_truth_w = 2.3 # slope
    ground_truth_b = -8 #intercept
    X = np.random.randn(m)*2
    Y = ground_truth_w*X + ground_truth_b + 0.2*np.random.randn(m)
    return X, Y #returns X (the input) and Y (labels)

def plot_data(X, Y):
    plt.figure()
    plt.scatter(X, Y, c=&#39;r&#39;)
    plt.xlabel(&#39;X&#39;)
    plt.ylabel(&#39;Y&#39;)
    plt.show()
    
m = 10
X, Y = sample_linear_data(m)
print(&#39;X:&#39;,X, &#39;\n&#39;)</code></pre>
<pre><code>## (&#39;X:&#39;, array([-1.63448237,  0.96415947, -0.07710786, -2.14409897,  0.83282966,
##        -1.0702922 , -1.65078261, -0.40474873,  2.2062286 , -1.43932678]), &#39;\n&#39;)</code></pre>
<pre class="python"><code>print(&#39;Y:&#39;,Y, &#39;\n&#39;)</code></pre>
<pre><code>## (&#39;Y:&#39;, array([-11.75597358,  -5.59727159,  -8.32278429, -12.71434132,
##         -5.98316411, -10.71690749, -11.97696556,  -8.82174929,
##         -2.81018873, -11.24533045]), &#39;\n&#39;)</code></pre>
<pre class="python"><code>plot_data(X, Y)</code></pre>
<p><img src="/post/machine-learning-foundations-with-python_files/figure-html/unnamed-chunk-1-1.png" /><!-- --></p>
<p>So now we have created our artificial data. Now lets create our own model and use it to make prediction on our data. We will be using a simple linear model which has a single weight and bias.</p>
<pre class="python"><code>class LinearHypothesis:
    def __init__(self): #initalize parameters 
        self.w = np.random.randn() #weight
        self.b = np.random.randn() #bias
    def __call__(self, X): #how do we calculate output from an input in our model?
        ypred = self.w*X + self.b
        return ypred
    def update_params(self, new_w, new_b):
        self.w = new_w
        self.b = new_b

H = LinearHypothesis()
y_hat = H(X)
print(&#39;Input:&#39;,X, &#39;\n&#39;)</code></pre>
<pre><code>## (&#39;Input:&#39;, array([-1.63448237,  0.96415947, -0.07710786, -2.14409897,  0.83282966,
##        -1.0702922 , -1.65078261, -0.40474873,  2.2062286 , -1.43932678]), &#39;\n&#39;)</code></pre>
<pre class="python"><code>print(&#39;W:&#39;, H.w, &#39;B:&#39;, H.b, &#39;\n&#39;)</code></pre>
<pre><code>## (&#39;W:&#39;, 1.953842412228336, &#39;B:&#39;, 0.32668471826241713, &#39;\n&#39;)</code></pre>
<pre class="python"><code>print(&#39;Prediction:&#39;, y_hat, &#39;\n&#39;)</code></pre>
<pre><code>## (&#39;Prediction:&#39;, array([-2.86683626,  2.21050039,  0.1760281 , -3.86254678,  1.95390264,
##        -1.76449758, -2.89868437, -0.46413051,  4.63730772, -2.48553299]), &#39;\n&#39;)</code></pre>
<p><strong>Lets visualise our hypothesis vs the labels</strong></p>
<pre class="python"><code>def plot_h_vs_y(X, y_hat, Y):
    plt.figure()
    plt.scatter(X, Y, c=&#39;r&#39;, label=&#39;Label&#39;)
    plt.scatter(X, y_hat, c=&#39;b&#39;, label=&#39;Hypothesis&#39;, marker=&#39;x&#39;)
    plt.legend()
    plt.xlabel(&#39;X&#39;)
    plt.ylabel(&#39;Y&#39;)
    plt.show()
    
plot_h_vs_y(X, y_hat, Y)</code></pre>
<p><img src="/post/machine-learning-foundations-with-python_files/figure-html/unnamed-chunk-3-1.png" /><!-- --></p>
<p><strong>How good our model is ?</strong></p>
<p>So lets calculate the cost. In this case we will use mean squared error.</p>
<pre class="python"><code>#loss function 
def L(y_hat, labels):
    cost = np.sum(np.square(y_hat-labels))/(m)
    return cost
cost = L(y_hat, Y)
print(cost)</code></pre>
<pre><code>## 71.8169051766</code></pre>
<p><strong>How to find the right weight values for our model?</strong><br />
There multiple technique like random search, grid search, gradient descent etc.</p>
<p><strong>Random Search</strong></p>
<pre class="python"><code>def random_search(n_samples):
    best_weights = None
    best_bias = None
    lowest_cost=100000 #initialize it very high
    for i in range(n_samples):
        H.update_params(np.random.randn(), np.random.randn())
        y_hat = H(X)
        cost = L(y_hat, Y)
        if cost&lt;lowest_cost:
            lowest_cost=cost
            best_weights = H.w
            best_bias = H.b
    print(&#39;Lowest cost of&#39;, lowest_cost, &#39;achieved with weight of&#39;, best_weights, &#39;and bias of&#39;, best_bias)
    return lowest_cost, best_weights
    
lowest_cost, best_weights = random_search(1000)</code></pre>
<pre><code>## (&#39;Lowest cost of&#39;, 34.196132911834489, &#39;achieved with weight of&#39;, 2.0183917505213107, &#39;and bias of&#39;, -2.2720820225376404)</code></pre>
<pre class="python"><code>
plot_h_vs_y(X, H(X), Y)</code></pre>
<p><img src="/post/machine-learning-foundations-with-python_files/figure-html/unnamed-chunk-6-1.png" /><!-- --></p>
<p><strong>Grid Search</strong></p>
<pre class="python"><code>from itertools import permutations
def generate_grid_search_values(n_params, n_samples=100, minval=-2.5, maxval=2.5):
    n_samples_per_param = np.power(n_samples, 1/n_params).round()
    param_values = np.linspace(-2.5, 2.5, n_samples_per_param)
    grid_samples = permutations(param_values, n_params)
    return grid_samples

def grid_search(grid_search_values):
    best_weights = None
    best_bias = None
    lowest_cost=100000 #initialize it very high
    for search_val in grid_search_values:
        H.update_params(search_val[0], search_val[1])
        y_hat = H(X)
        cost = L(y_hat, Y)
        if cost&lt;lowest_cost:
            lowest_cost=cost
            best_weights = H.w
            best_bias = H.b
    print(&#39;Lowest cost of&#39;, lowest_cost, &#39;achieved with weight of&#39;, best_weights, &#39;and bias of&#39;, best_bias)
    return lowest_cost, best_weights</code></pre>
<pre class="python"><code>grid_search_values = list(generate_grid_search_values(2, n_samples=1000))
lowest_cost, best_weights = grid_search(grid_search_values)</code></pre>
<pre><code>## (&#39;Lowest cost of&#39;, 100000, &#39;achieved with weight of&#39;, None, &#39;and bias of&#39;, None)</code></pre>
<pre class="python"><code>plot_h_vs_y(X, H(X), Y)</code></pre>
<p><img src="/post/machine-learning-foundations-with-python_files/figure-html/unnamed-chunk-8-1.png" /><!-- --></p>
<p><strong>Gradient Descent</strong><br />
Gradient descent is another optimization algorithm that we could use. We can use gradient descent when our model is a differentiable function. Linear functions are pretty simple to differentiate hence why we can use it here.</p>
<p>The algorithm starts by randomly initializing our parameters. We then calculate the cost of those parameters and the derivative of our cost w.r.t each parameter. This tells us the direction of steepest ascent. We update each parameter value by taking a step in the opposite direction, proportional to the learning rate.</p>
<pre class="python"><code>class LinearHypothesis:
    def __init__(self): 
        self.w = np.random.randn() #weight
        self.b = np.random.randn() #bias
    def __call__(self, X): #how do we calculate output from an input in our model?
        y_hat = self.w*X + self.b
        return y_hat
    def update_params(self, new_w, new_b):
        self.w = new_w
        self.b = new_b
    def calc_deriv(self, X, y_hat, labels):
        diffs = y_hat-labels
        dLdw = 2*np.array(np.sum(diffs*X)/m)
        dLdb = 2*np.array(np.sum(diffs)/m)
        return dLdw, dLdb</code></pre>
<pre class="python"><code>H = LinearHypothesis()
y_hat = H(X)
dLdw, dLdb = H.calc_deriv(X, y_hat, Y)
print(dLdw, dLdb)</code></pre>
<pre><code>## (-11.809172231970177, 17.346381865845107)</code></pre>
<p>Now that we can complete the derivatives, complete the train function below to iteratively improve our parameter estimes to minimize the cost</p>
<pre class="python"><code>num_epochs = 200
learning_rate = 0.1
H = LinearHypothesis()</code></pre>
<pre class="python"><code>def train(num_epochs, X, Y, H, L, plot_cost_curve=False):
    all_costs = []
    for e in range(num_epochs):
        y_hat = H(X)
        cost = L(y_hat, Y)
        dLdw, dLdb = H.calc_deriv(X, y_hat, Y)
        new_w = H.w - learning_rate*dLdw
        new_b = H.b - learning_rate*dLdb
        H.update_params(new_w, new_b)
        all_costs.append(cost)
    if plot_cost_curve:
        plt.figure()
        plt.ylabel(&#39;Cost&#39;)
        plt.xlabel(&#39;Epoch&#39;)
        plt.plot(all_costs)
    print(&#39;Final cost:&#39;, cost)
    print(&#39;Weight values:&#39;, H.w)
    print(&#39;Bias values:&#39;, H.b)
    #return cost, H.w</code></pre>
<pre class="python"><code>train(num_epochs, X, Y, H, L, plot_cost_curve=True)</code></pre>
<pre><code>## (&#39;Final cost:&#39;, 0.021611789851737503)
## (&#39;Weight values:&#39;, 2.3327032524983231)
## (&#39;Bias values:&#39;, -7.9639675672661916)</code></pre>
<pre class="python"><code>plot_h_vs_y(X, H(X), Y)</code></pre>
<p><img src="/post/machine-learning-foundations-with-python_files/figure-html/unnamed-chunk-14-1.png" /><!-- --></p>
</div>
