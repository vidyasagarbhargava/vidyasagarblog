<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R on Vidyasagar Bhargava</title>
    <link>/categories/r/</link>
    <description>Recent content in R on Vidyasagar Bhargava</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Jun 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/r/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Gradient Descent from scratch and visualization</title>
      <link>/2019/06/gradient-descent-from-scratch-and-visualization/</link>
      <pubDate>Fri, 14 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/06/gradient-descent-from-scratch-and-visualization/</guid>
      <description>Gradient descent is an optimization algorithm used to find the values of parameters (coefficients) of a function (f) that minimizes a cost function. There are alot of blogs and videos on youtube about this topic. So therefore we are not going into theory here.
Below video discusses about gradient descent algorithm. Its good have spend some time on this video and get intuition about it before we start writing code for it.</description>
    </item>
    
    <item>
      <title>Softmax function from scratch</title>
      <link>/2019/06/softmax-function-from-scratch/</link>
      <pubDate>Fri, 14 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/06/softmax-function-from-scratch/</guid>
      <description>Softmax is a generalization of logistic regression which can be use for multi-class classification. The softmax function squashes the outputs of each unit to be between 0 and 1, just like a sigmoid function. But it also divides each output such that the total sum of the outputs is equal to 1.
Softmax Function :-
Softmax is a generalization of logistic regression which can be use for multi-class classification.</description>
    </item>
    
    <item>
      <title>Tidyverse Style titanic kaggle competition</title>
      <link>/2019/06/tidyverse-style-titanic-kaggle-competition/</link>
      <pubDate>Mon, 03 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/06/tidyverse-style-titanic-kaggle-competition/</guid>
      <description>This post is an effort of showing a new approach of modelling in R using tidyverse and tidymodels.We will go through step by step from data import to final model evaluation process in machine learning. We will not just focus on coding part but also the statistical aspect should be taken into account behind the modelling process. In this tutorial we are using titanic dataset from Kaggle competition.</description>
    </item>
    
  </channel>
</rss>