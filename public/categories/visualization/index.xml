<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>visualization on Vidyasagar Bhargava</title>
    <link>/categories/visualization/</link>
    <description>Recent content in visualization on Vidyasagar Bhargava</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Jun 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/visualization/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Gradient Descent from scratch and visualization</title>
      <link>/2019/06/gradient-descent-from-scratch-and-visualization/</link>
      <pubDate>Fri, 14 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/06/gradient-descent-from-scratch-and-visualization/</guid>
      <description>Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model. Parameters refer to coefficients in Linear Regression and weights in neural networks.
Below video discusses about gradient descent algorithm. Its good to spend some time on this video and get intuition about it before we start writing code for it.</description>
    </item>
    
  </channel>
</rss>