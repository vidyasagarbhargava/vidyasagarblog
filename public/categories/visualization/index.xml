<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>visualization on Vidyasagar Bhargava</title>
    <link>/categories/visualization/</link>
    <description>Recent content in visualization on Vidyasagar Bhargava</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Jun 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/visualization/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Gradient Descent from scratch and visualization</title>
      <link>/2019/06/gradient-descent-from-scratch-and-visualization/</link>
      <pubDate>Fri, 14 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/06/gradient-descent-from-scratch-and-visualization/</guid>
      <description>Gradient descent is an optimization algorithm used to find the values of parameters (coefficients) of a function (f) that minimizes a cost function. There are alot of blogs and videos on youtube about this topic. So therefore we are not going into theory here.
In this video from coursera machine learning course Andrew NG discusses about gradient descent algorithm. Its good have spend some time on this video and get intuition about it before we start writing code for it.</description>
    </item>
    
  </channel>
</rss>